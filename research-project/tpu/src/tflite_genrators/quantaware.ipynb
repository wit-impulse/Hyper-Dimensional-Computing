{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantaware.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAZYXvZU_XG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4yVFK5-0Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740f672e-3681-4ba7-d500-3013c9b1a162"
      },
      "source": [
        "! pip install -q tensorflow\n",
        "! pip install -q tensorflow-model-optimization\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 213 kB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "source": [
        "import tempfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp"
      },
      "source": [
        "## Train a model for MNIST without quantization aware training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOULkunkOKmT"
      },
      "source": [
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "#y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVs1EPwOJ2z"
      },
      "source": [
        "# ----------------------------------------CNN1-UNCOMMENT BELOW CODE-------------------------------------------------------------\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "\n",
        "  tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "  tf.keras.layers.Conv2D(filters=32, kernel_size=(2,2), padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "#-----------------------------------------------CNN2-UNCOMMENT BELOW CODE----------------------------------------------------------\n",
        "\n",
        "# model = tf.keras.Sequential()\n",
        "\n",
        "# model.add(tf.keras.layers.InputLayer(input_shape=(28, 28)))\n",
        "# model.add(tf.keras.layers.Reshape(target_shape=(28, 28, 1)))\n",
        "# model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))       \n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "# model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(tf.keras.layers.BatchNormalization()),\n",
        "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "# model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "# model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "# tf.keras.layers.BatchNormalization(),\n",
        "# model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "# model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "# model.add(tf.keras.layers.Flatten())\n",
        "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "# model.add(tf.keras.layers.Dropout(0.4))\n",
        "# model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ2_ZC8LOOuZ"
      },
      "source": [
        "# Train the digit classification model\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8747K9OE72P"
      },
      "source": [
        "## Clone and fine-tune pre-trained model with quantization aware training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F19k7ExXF_h2"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsZROpNYMWQ0"
      },
      "source": [
        "You will apply quantization aware training to the whole model and see this in the model summary. All layers are now prefixed by \"quant\".\n",
        "\n",
        "Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8). The sections after show how to create a quantized model from the quantization aware one.\n",
        "\n",
        "In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md), you can see how to quantize some layers for model accuracy improvements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq6blGjgFDCW"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr2ijwpGCI-"
      },
      "source": [
        "### Train and evaluate the model against baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUBEn94hXYB1"
      },
      "source": [
        "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PHDGJryE31X"
      },
      "source": [
        "train_images_subset = train_images[0:1000] # out of 60000\n",
        "train_labels_subset = train_labels[0:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=64, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-byC2lYlMkfN"
      },
      "source": [
        "For this example, there is minimal to no loss in test accuracy after quantization aware training, compared to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bMFTKSSHyyZ"
      },
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IepmUPSITn6"
      },
      "source": [
        "## Create quantized model for TFLite backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FgNP4rbOLH8"
      },
      "source": [
        "After this, you have an actually quantized model with int8 weights and uint8 activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7fztWsAOHTz"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYsyYVqNgeY"
      },
      "source": [
        "## See persistence of accuracy from TF to TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saadXD4JQsBK"
      },
      "source": [
        "Define a helper function to evaluate the TF Lite model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_U9SR5dvXgJ"
      },
      "source": [
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4IrcNnvbzM"
      },
      "source": [
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8yBouuGNqls"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuEFS4CIQvUw"
      },
      "source": [
        "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqQTyqz4NsWd"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy*100)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save tflite file"
      ],
      "metadata": {
        "id": "IaP0uNbQT-OR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKyp55rxBXP3"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"fmnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"quantization_aware_training.tflite\"\n",
        "tflite_model_quant_file.write_bytes(quantized_tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8D7WnFF5DZR"
      },
      "source": [
        "## See 4x smaller model from quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1c2IecBRCdQ"
      },
      "source": [
        "You create a float TFLite model and then see that the quantized TFLite model\n",
        "is 4x smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy_Lgfh8VkyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cb1d8b-3c8a-4ce0-d2c1-5239999267cb"
      },
      "source": [
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpoj1lb_y7/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpoj1lb_y7/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model in Mb: 1.5784797668457031\n",
            "Quantized model in Mb: 0.40206146240234375\n"
          ]
        }
      ]
    }
  ]
}